Mathematically, an optimization problem is defined by an objective function  f(x)  which depends on a set of variables  x  (parameters). Training a machine learning model often boils down to finding a good set of parameters. The notion of “good” is determined by the objective function, in particular finding its best value using optimization algorithms. Assuming that object function is differentiable (so we have acces to a gradient at each location in the space), we can optimize the algorithm by minimizing our "loss function". Intuitively finding the best value is like finding the valleys of the function, moving in the opposite direction of the gradient(which points uphill). Basing on this concept we can use gradient descent algorithm to solve linear system of equations.
